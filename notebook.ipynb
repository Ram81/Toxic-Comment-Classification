{
  "cells": [
    {
      "metadata": {
        "_uuid": "f6cbd47eadccedab5566ba4502801f85644e475c",
        "_cell_guid": "ed348e31-67d7-4385-ab76-31eb4a5ffced",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "131b0485cbc80136db6d0167f5718d1332d43639",
        "_cell_guid": "38105b59-280d-4d59-91b1-4c2a5251f778",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nimport os, re, csv, math, codecs\n\nsns.set_style(\"whitegrid\")\nnp.random.seed(0)\n\nDATA_PATH = '../input/'\nEMBEDDING_PATH = '../input/'\n\nMAX_NB_WORDS = 100000\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ce2fad133f07b7dde3282fe15b8059e8794b582",
        "_cell_guid": "18588ba9-70f1-4278-aed9-4f1c21a0818a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load Embedding\nprint('load word embedding...')\nembedding_index = {}\nf = codecs.open('../input/fasttext/wiki.simple1.vec', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_index[word] = coefs\nf.close()\nprint('found %s word vectors' % len(embedding_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9b6b1a0f7318fd01f33899feb590bba414e1cdd5",
        "_cell_guid": "759ce5d6-dbc5-41e3-a8cd-ef8018ee4bde",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load Data\ninput_dir = '../input/jigsaw-toxic-comment-classification-challenge/'\ntrain_df = pd.read_csv(input_dir + 'train.csv', sep=',', header=0)\ntest_df = pd.read_csv(input_dir + 'test.csv', sep=',', header=0)\n\ntest_df = test_df.fillna('_NA_')\nprint('Training DF: '+str(train_df.shape))\nprint('Testing DF: '+str(test_df.shape))\n\nlabel_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny_train = train_df[label_names].values\n\nprint(y_train.shape)\n\n# Visualize\ntrain_df['doc_len'] = train_df['comment_text'].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\nsns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc_len')\nplt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\nplt.title('comment length');\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d48b9331c51a9e1a7898e5c96408b6d3f0d69fd",
        "_cell_guid": "2d0207eb-54d6-4e78-a629-e6fbae4b6ef4",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "raw_docs_train = train_df['comment_text'].tolist()\nraw_docs_test = test_df['comment_text'].tolist()\nnum_classes = len(label_names)\n\nprint('pre-processing training data')\nprocessed_train_data = []\nfor doc in tqdm(raw_docs_train):\n    tokens = tokenizer.tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_train_data.append(\" \".join(filtered))\n\nprint('pre-processing test data')\nprocessed_test_data = []\nfor doc in tqdm(raw_docs_test):\n    tokens = tokenizer.tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_test_data.append(\" \".join(filtered))\n\nprint(\"Tokenizing input data....\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_train_data + processed_test_data)\nword_seq_train = tokenizer.texts_to_sequences(processed_train_data)\nword_seq_test = tokenizer.texts_to_sequences(processed_test_data)\nword_index = tokenizer.word_index\nprint(\"Dictionary Size: \"+str(len(word_index)))\n\n# Pad Sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a2c7a4b78c3f85e2a4813c7b0c888601ba15815",
        "_cell_guid": "db96b6ea-c566-4a72-96b5-05ecd5410b77",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Training Parameters\nbatch_size = 256\nnum_epochs = 8\n\nnum_filters = 64\nembed_dim = 300\nweight_decay = 1e-4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61ca02a1d696bb9490e2f627b76a1443e301655e",
        "_cell_guid": "3f94e549-f7a7-4c15-bb03-6f8a6574e5cb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Embedding Matrix\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_dim))\n\nfor word, i in word_index.items():\n    if(i>=nb_words):\n        continue\n    embedding_vector = embedding_index.get(word)\n    if(embedding_vector is not None) and len(embedding_vector)>0:\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('Number of null words embeddings found %d' % np.sum(np.sum(embedding_matrix, axis=1)==0))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4be9e1f5ade7ed2b05a57a53477bd017b9138a3c",
        "_cell_guid": "ad9377b0-13a4-410b-90a7-e56a533655da",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(' Sample words not found: ',np.random.choice(words_not_found, 10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "019e731862f07750276313321ddd3a0269861520",
        "_cell_guid": "10f28c23-c23f-4bce-8aad-d52087cf151e",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Building CNN')\nmodel = Sequential()\nmodel.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(num_classes, activation='sigmoid'))\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fdd54ca8eb912169c3816b074ef572ff59592b0",
        "_cell_guid": "a52d6746-a594-43ce-a1e6-2da394378e00",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallback_list = [early_stopping]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53897328805acb3277e9e2ea8a613222300ccab2",
        "_cell_guid": "8b7704c9-d95b-446b-b8df-310b3dc6ca4f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Training CNN\nhist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callback_list, validation_split=0.1, shuffle=True, verbose=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "330dbbd5292cb0f77cda43f3a40c2051e01eac32",
        "_cell_guid": "f840345e-28f0-4976-94a2-c875482f1609",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "y_test = model.predict(word_seq_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1de72c194c733c2588c3b33d35a0dc2df40cfc0a",
        "_cell_guid": "57b10854-4e04-44cb-a7c2-9880e0451eaa",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#create a submission\nsubmission_df = pd.DataFrame(columns=['id'] + label_names)\nsubmission_df['id'] = test_df['id'].values \nsubmission_df[label_names] = y_test \nsubmission_df.to_csv(\"./submit.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b756ff2e1edf0c44b867a34218bd79bf0e34b4ce",
        "_cell_guid": "4bb57419-ef30-46fd-a8c8-e06e1195e377",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Generate Plots\nplt.figure()\nplt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN Toxic Sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9fbee0a2f73fbe942745e5a42a5324c62cde1b43",
        "_cell_guid": "97940a79-621b-407b-9f7c-c1cf2566da0a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure()\nplt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('CNN Toxic Sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "name": "python",
      "file_extension": ".py",
      "nbconvert_exporter": "python",
      "version": "3.6.3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}